
# プロバイダマトリクス

このページでは、Agno-Go がサポートする各モデルプロバイダの機能と設定ポイントを高いレベルで比較します。以下の点を素早く把握することが目的です。

- どのプロバイダがチャット（chat）に利用できるか  
- どのプロバイダがベクトル埋め込み（embedding）をサポートするか（対応状況による）  
- どのプロバイダがストリーミング出力をサポートするか  
- 各プロバイダで設定すべき環境変数は何か  

> 実際に利用できるモデルや機能は、各プロバイダでのアカウント、リージョン、クォータに依存します。ここに示す内容は Go アダプタと `.env.example` に基づくものであり、詳細は必ず各プロバイダの公式ドキュメントを参照してください。

## サマリーテーブル

| プロバイダ   | チャットサポート             | 埋め込みサポート                 | ストリーミングサポート            | 主な環境変数                                                                      |
|--------------|------------------------------|----------------------------------|------------------------------------|-----------------------------------------------------------------------------------|
| OpenAI       | 対応（chat）                | 対応（embeddings）               | 対応（chat ストリーミング）       | `OPENAI_API_KEY`, `OPENAI_ENDPOINT`, `OPENAI_ORG`, `OPENAI_API_VERSION`           |
| Gemini       | 対応（chat）                | 対応（embeddings）               | 対応（chat ストリーミング）       | `GEMINI_API_KEY`, `GEMINI_ENDPOINT`, `VERTEX_PROJECT`, `VERTEX_LOCATION`          |
| GLM4         | 対応（chat）                | 制限あり / 計画中*               | モデルにより異なる                 | `GLM4_API_KEY`, `GLM4_ENDPOINT`                                                   |
| OpenRouter   | 対応（chat/ルーティング）   | 下位モデルが対応している場合のみ | 下位モデルが対応している場合のみ   | `OPENROUTER_API_KEY`, `OPENROUTER_ENDPOINT`                                       |
| SiliconFlow  | 対応（chat）                | 対応（embeddings）               | 対応（chat ストリーミング）       | `SILICONFLOW_API_KEY`, `SILICONFLOW_ENDPOINT`                                     |
| Cerebras     | 対応（chat）                | 公式サポート状況による           | 公式サポート状況による             | `CEREBRAS_API_KEY`, `CEREBRAS_ENDPOINT`                                           |
| ModelScope   | 対応（chat）                | 公式サポート状況による           | 公式サポート状況による             | `MODELSCOPE_API_KEY`, `MODELSCOPE_ENDPOINT`                                       |
| Groq         | 対応（chat）                | 制限あり / 計画中*               | 対応（chat ストリーミング）       | `GROQ_API_KEY`, `GROQ_ENDPOINT`                                                   |
| Ollama       | 対応（ローカル chat）       | ローカルモデルの実装に依存       | 対応（ローカル chat ストリーミング）| `OLLAMA_ENDPOINT`                                                                 |

`*` 一部プロバイダの embedding サポートは現在も進化中です。まだ完全にサポートされていない、または一部のモデルに限定されている場合、Go アダプタはテスト時にサポートされていない呼び出しをスキップするか、契約ドキュメントに差異を明記します。

## 設定メモ

- プロバイダ関連の環境変数はすべて `.env.example` に記載されています。これを `.env` にコピーし、実際に利用したいプロバイダだけに値を設定してください。  
- 必須キーが空のままの場合、ヘルスチェックやプロバイダテストはそのプロバイダをスキップし、その理由を明示します。ランタイムは未設定のプロバイダを自動的には呼び出しません。  
- `OPENAI_ENDPOINT` や `GEMINI_ENDPOINT` などの変数はデフォルトで公式ホスト API を指しますが、必要に応じてプライベートゲートウェイやプロキシに変更できます。  
- `OLLAMA_ENDPOINT` は通常、ローカルで動作する Ollama/vLLM インスタンス（例：`http://localhost:11434/v1`）を指し、ローカルモデルを明示的に有効化した場合にのみ使用されます。  

ルーティングロジックやエラー規約については、**Core Features & API 概要** ページおよび specs ディレクトリ内の契約文書と併せて確認してください。

## 次のステップ

- 本ページで挙げた環境変数の意味やキー管理の推奨プラクティスについては、[設定とセキュリティ](../config-and-security) のページで詳しく説明しています。  
- Quickstart の例を別のプロバイダ構成で試したい場合は、[クイックスタート](../quickstart) に戻り、このマトリクスを参考に設定を調整してください。  
